kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: v1
provisioner: microk8s.io/hostpath
reclaimPolicy: Retain
parameters:
  pvDir: /mnt/v1
volumeBindingMode: WaitForFirstConsumer
---
apiVersion: elasticsearch.k8s.elastic.co/v1
kind: Elasticsearch
metadata:
  name: elasticsearch
spec:
  version: 8.14.1
  nodeSets:
    - name: default
      count: 3
      volumeClaimTemplates:
        - metadata:
            name: elasticsearch-data
          spec:
            accessModes:
              - ReadWriteOnce
            resources:
              requests:
                storage: 20Gi
            storageClassName: v1
      podTemplate:
        metadata:
          labels:
            app: elasticsearch
        spec:
          containers:
            - name: elasticsearch
              resources:
                limits:
                  memory: 3Gi
                #requests:
                #  memory: 1Gi
          #affinity:
          #  nodeAffinity:
          #    requiredDuringSchedulingIgnoredDuringExecution:
          #      nodeSelectorTerms:
          #        - matchExpressions:
          #            - key: kubernetes.io/hostname
          #              operator: NotIn
          #              values:
          #                - microk8s-1
---
apiVersion: kibana.k8s.elastic.co/v1
kind: Kibana
metadata:
  name: kibana
spec:
  version: 8.14.1
  count: 1
  elasticsearchRef:
    name: elasticsearch
  podTemplate:
    metadata:
      labels:
        app: kibana
#    spec:
#      containers:
#        - name: kibana
#          resources:
#            limits:
#              memory: 1Gi
#          #requests:
#          #  memory: 1Gi
---
apiVersion: logstash.k8s.elastic.co/v1alpha1
kind: Logstash
metadata:
  name: logstash
spec:
  count: 3
  version: 8.14.1
  elasticsearchRefs:
    - clusterName: eck
      name: elasticsearch
  podTemplate:
    metadata:
      labels:
        app: logstash
    spec:
      securityContext:
        runAsUser: 0
      containers:
        - name: logstash
          resources:
            limits:
              memory: 2Gi
    #          #requests:
    #          #  memory: 1Gi
          env:
            - name: KAFKA_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: kafka-user-passwords
                  key: client-passwords
  pipelines:
    - pipeline.id: main
      config.string: |
        input {
          beats {
            port => 5044
          }
        }
        filter {
          if [data_stream][dataset] == "kubernetes-container" {
            if [kubernetes][namespace] {
              mutate {
                replace => { "[data_stream][namespace]" => "%{[kubernetes][namespace]}" }
              }
            } else {
              mutate {
                replace => { "[data_stream][namespace]" => "unknown" }
              }
            }
          } 
          else if [data_stream][dataset] == "kubernetes-audit" {
            json {
              source => "message"
              target => "[kubernetes][audit]"
            }
            ruby {
              init => "
                require 'json'
                def process_audit_data(event)
                  ['requestObject', 'responseObject'].each do |field|
                    obj = event.get('[kubernetes][audit][' + field + ']')
                    if obj.is_a?(Hash) || obj.is_a?(Array)
                      json_str = obj.to_json
                      event.set('[kubernetes][audit][' + field + ']', json_str)
                    end
                  end
                end
              "
              code => "
                process_audit_data(event)
              "
            }
            if [kubernetes][audit][objectRef][namespace] {
              mutate {
                add_field => { "[data_stream][namespace]" => "%{[kubernetes][audit][objectRef][namespace]}" }
              }
            } else {
              mutate {
                add_field => { "[data_stream][namespace]" => "unknown" }
              }
            }
          }
          else if [data_stream][dataset] == "kubernetes-netflow-hubble" {
            json {
              source => "message"
              target => "[hubble]"
            }
            if [hubble][flow][source][namespace] {
              mutate {
                replace => { "[data_stream][namespace]" => "%{[hubble][flow][source][namespace]}" }
              }
            } else if [hubble][flow][destination][namespace] {
              mutate {
                replace => { "[data_stream][namespace]" => "%{[hubble][flow][destination][namespace]}" }
              }
            } else if [hubble][agent_event][service_upsert][namespace] {
              mutate {
                replace => { "[data_stream][namespace]" => "%{[hubble][agent_event][service_upsert][namespace]}" }
              }
            } else {
              mutate {
                replace => { "[data_stream][namespace]" => "unknown" }
              }
            }
          }
          else if [data_stream][dataset] == "kubernetes-ids-tetragon" {
            json {
              source => "message"
              target => "[tetragon]"
            }
            ruby {
              init => "
                def process_tetragon_data(event)
                  tetragon_data = event.get('[tetragon]')
                  if tetragon_data.is_a?(Hash)
                    tetragon_data.each do |key, value|
                      if value.is_a?(Hash) && value.key?('process') && value['process'].key?('pod') && value['process']['pod'].key?('namespace')
                        namespace = value['process']['pod']['namespace']
                        event.set('[data_stream][namespace]', namespace)
                        return event
                      end
                    end
                  end
                  event.set('[data_stream][namespace]', 'unknown')
                end
              "
              code => "
                process_tetragon_data(event)
              "
            }
          }
        }
        output {
          elasticsearch {
            hosts => [ "${ECK_ES_HOSTS}" ]
            user => "${ECK_ES_USER}"
            password => "${ECK_ES_PASSWORD}"
            ssl_certificate_authorities => "${ECK_ES_SSL_CERTIFICATE_AUTHORITY}"
          }
        }
    - pipeline.id: kafka
      config.string: |
        input {
          kafka {
            bootstrap_servers => "kafka.elastic-system.svc.cluster.local:9092"
            topics => ["falco-events"]
            group_id => "logstash"
            consumer_threads => 3
            decorate_events => true
            sasl_mechanism => "PLAIN"
            security_protocol => "SASL_PLAINTEXT"
            sasl_jaas_config => 'org.apache.kafka.common.security.plain.PlainLoginModule required username="user1" password="${KAFKA_PASSWORD}";'
          }
        }
        filter {
          json {
            source => "message"
            target => "falco"
          }
          if [falco][output_fields][k8s.ns.name] {
            mutate {
              replace => { "[data_stream][namespace]" => "%{[falco][output_fields][k8s.ns.name]}" }
            }
          } else {
            mutate {
              replace => { "[data_stream][namespace]" => "unknown" }
            }
          }
        }
        output {
          elasticsearch {
            hosts => [ "${ECK_ES_HOSTS}" ]
            user => "${ECK_ES_USER}"
            password => "${ECK_ES_PASSWORD}"
            data_stream => true
            data_stream_type => "logs"
            data_stream_dataset => "kubernetes-ids-falco"
            #ilm_rollover_alias => "kubernetes.falco"
            #ilm_pattern => "{now/d}-000001"
            #ilm_policy => "falco-events-policy"
            ssl_certificate_authorities => "${ECK_ES_SSL_CERTIFICATE_AUTHORITY}"
          }
        }
  services:
    - name: beats
      service:
        spec:
          type: ClusterIP
          ports:
            - port: 5044
              name: "filebeat"
              protocol: TCP
              targetPort: 5044
--- # the service accocunt and cluster role binding for the filebeat are required, otherwise the filebeat will not be able to access the kubernetes API and the enrichment add_kubernetes_metadata will not work
apiVersion: v1
kind: ServiceAccount
metadata:
  name: elastic-beat-filebeat
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: elastic-beat-autodiscover-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: elastic-beat-autodiscover
subjects:
- kind: ServiceAccount
  name: elastic-beat-filebeat
  namespace: elastic-system
---
apiVersion: beat.k8s.elastic.co/v1beta1
kind: Beat
metadata:
  name: filebeat
spec:
  type: filebeat
  version: 8.14.1
  config:
    incluster: true
    filebeat.inputs:
      - type: container
        paths:
          - /var/log/containers/*.log
        data_stream:
          dataset: kubernetes-container
        processors:
        - add_kubernetes_metadata:
            host: ${NODE_NAME}
            matchers:
            - logs_path:
                logs_path: '/var/log/containers/'
        - add_fields:
            target: data_stream
            fields:
              dataset: kubernetes-container
      - type: log
        paths:
          - /var/log/apiserver/audit.log
        processors:
        - add_fields:
            target: ""
            fields:
              data_stream:
                dataset: kubernetes-audit
              kubernetes:
                node:
                  name: ${NODE_NAME}
      - type: log
        paths:
          - /var/log/hubble/events.log
        processors:
        - add_fields:
            target: ""
            fields:
              data_stream:
                dataset: kubernetes-netflow-hubble
      - type: log
        paths:
          - /var/log/tetragon/tetragon.log
        processors:
        - add_fields:
            target: ""
            fields:
              data_stream:
                dataset: kubernetes-ids-tetragon
    output.logstash:
      hosts: ["logstash-ls-beats:5044"]
  daemonSet:
    podTemplate:
      metadata:
        labels:
          app: filebeat
      spec:
        dnsPolicy: ClusterFirstWithHostNet
        serviceAccount: elastic-beat-filebeat
        automountServiceAccountToken: true
        hostNetwork: false
        securityContext:
          runAsUser: 0
        containers:
        - name: filebeat
          volumeMounts:
          - name: varlogcontainers
            mountPath: /var/log/containers
          - name: varlogpods
            mountPath: /var/log/pods
          - name: varlibdockercontainers
            mountPath: /var/lib/docker/containers
          - name: varlogapiserver
            mountPath: /var/log/apiserver
          - name: varloghubble
            mountPath: /var/log/hubble
          - name: varlogtetragon
            mountPath: /var/log/tetragon
          env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
        volumes:
        - name: varlogcontainers
          hostPath:
            path: /var/log/containers
        - name: varlogpods
          hostPath:
            path: /var/log/pods
        - name: varlibdockercontainers
          hostPath:
            path: /var/lib/docker/containers
        - name: varlogapiserver
          hostPath:
            path: /var/log/apiserver
        - name: varloghubble
          hostPath:
            path: /var/snap/microk8s/current/var/run/cilium/hubble
        - name: varlogtetragon
          hostPath:
            path: /run/cilium/tetragon